# Capstone Weekly Report

### Date: Sep 03, 2024
- **Topics of Discussion:**
    - **RAG (Retrieval-Augmented Generation):** Understanding the framework and its applications.
    - **LangChain:** Introduction to the library and its role in RAG systems.
    - **Multi-Agent System:** Exploration of how multiple agents can be used to enhance the performance of language models.

- **Action Items:**
    * [&check;] Deep dive into RAG architecture.
    * [&check;] Explore practical use cases for LangChain in RAG.
    * [&check;] Research potential benefits of multi-agent systems in language models.

---

### Date: Sep 10, 2024
- **Topics of Discussion:**
    - **Project Workflow:** Created a general diagram to outline the workflow and architecture.
    - **Dataset Identification:** Found a suitable dataset for the project.
    - **RAG Pipeline:** Built a simple RAG pipeline using basic components.
    - **Model Experimentation:** Tested different models for embeddings and evaluated their effectiveness.
    - **LLM Integration:** Integrated an initial version of a Language Model (LLM) into the pipeline.

![Basic RAG Diagram](Basic_Rag.png)

- **Action Items:**
    * [&check;] Refine the project workflow diagram.
    * [&check;] Conduct more experiments with different datasets.
    * [&check;] Optimize the RAG pipeline with advanced components.
    * [&check;] Continue model experimentation for improved embeddings.
    * [&check;] Enhance LLM integration based on feedback.

---

### Date: Sep 17, 2024
- **Topics of Discussion:**
    - **Literature Review:** Plan to review relevant research related to RAG and multi-agent systems.
    - **Modular Code Structure:** Need to develop a code structure that supports scalability and testing.
    - **Testing Scripts:** Write and run test scripts to validate code functionality.
    - **Data Acquisition:** Start the process of data collection and cleaning.
    - **Data Exploration:** Continue testing data from various sources for suitability.

- **Action Items:**
    * [&check;] Conduct a thorough literature review on RAG and multi-agent systems.
    * [&check;] Design and implement a modular code structure.
    * [&check;] Develop and run test scripts for the codebase.
    * [&check;] Initiate data acquisition and cleaning process.
    * [&check;] Evaluate and test data from different sources for the project.

---

### Date: Sep 24, 2024
- **Topics of Discussion:**
    - **Test Folder Structure and Script Actions:** Refining the test folder structure and defining actions for testing vector store and embedding model.
    - **Main Folder Setup:** Creating a main folder to organize the main functions and core components of the project.
    - **PDF Parser Integration:** Integrating a PDF parser into the system to handle document processing.
    - **PDF Download Function:** Developing a Python function to download PDF documents from links mentioned in the data source of BioASQ - Task B.

- **Action Items:**
    * [&check;] Refine the folder structure for test scripts, specifically for vector store and embedding model.
    * [&check;] Define specific actions and test cases for validating vector store and embedding model functionalities.
    * [&check;] Create a main folder for organizing core functions and essential project components.
    * [&check;] Integrate a PDF parser into the system for handling document processing.
    * [&check;] Develop a Python function to download PDF documents from links mentioned in BioASQ - Task B data source.
    * [&check;] Test the PDF download function with a sample link and validate the parser's integration.

---


### Date: Oct 1, 2024
- **Topics of Discussion:**
    - **Score RAG:** Evaluating the performance of the Retrieval-Augmented Generation (RAG) system by scoring its results.
    - **Embedding Models:** Trying different models for embedding to optimize the RAG system's performance.
    - **Chunking Method:** Experimenting with various chunking methods to improve text processing and retrieval efficiency.
    - **RAG Pipeline:** Developing a complete pipeline for RAG with comprehensive testing for all its components.
    - **F-1 Score Evaluation:** Measuring the F-1 score of retrieval at different values of k to evaluate precision and recall in the retrieval process.

- **Action Items:**
    * [ ] Score RAG to assess its overall performance.
    * [ ] Try different models for embedding and compare their results.
    * [&check;] Experiment with different chunking methods to optimize the chunk size and method.
    * [&check;] Build a complete pipeline for the RAG system, ensuring all components are tested and functioning.
    * [ ] Test and record the F-1 score of retrieval at different values of k (e.g., k=5, 10, 20) to assess retrieval quality.

---

### Date: Oct 8, 2024
- **Topics of Discussion:**
    - **Embedding Comparison:** Plan to compare embeddings from NVIDIA and other open-source models.
    - **Data Preparation:** Ensure that at least 50 undamaged PDFs are available for testing.
    - **Embedding Evaluation:** Test and evaluate the performance of different embeddings on the prepared PDFs in table format.
    - **Performance Metrics:** Define and use key metrics (e.g., accuracy, speed, memory usage) to compare embeddings.
    - **Documentation:** Properly document the embedding comparison process and results.

- **Action Items:**
    * [ ] Gather and prepare at least 50 not-damaged PDFs for testing.
    * [ ] Implement the embedding process using both NVIDIA and open-source models.
    * [ ] Define performance metrics for embedding evaluation in table format.
    * [ ] Run embedding tests and evaluate results using the defined metrics.
    * [ ] Document the process and findings from the embedding comparison.

---

### Date: Oct 22, 2024
- **Topics of Discussion:**
    - **Generation Model Development:** Focus on improving the generation step of the RAG pipeline, ensuring relevance and coherence in the generated outputs.
    - **Evaluation of Embedding, Retrieval, and Generation Models:** Plan to evaluate the embedding, retrieval, and generation stages using key metrics (e.g., embedding accuracy, retrieval F1 score, generation BLEU score).
    - **Proposal for New RAG Application:** Discuss a new potential RAG application for real-time legal document summarization, leveraging RAG's capability for precise document retrieval and coherent summary generation.
    - **Reporting and Visualization:** Begin the report on RAG evaluation, documenting results and creating a diagram to illustrate the evaluation flow and proposed application.

- **Action Items:**
    * [ ] Improve coherence and relevance of generated outputs in the RAG pipeline.
    * [ ] Define and finalize the evaluation metrics for embedding, retrieval, and generation stages.
    * [ ] Run embedding, retrieval, and generation tests, then evaluate the results using the defined metrics.
    * [ ] Draft a proposal for using RAG in real-time legal document summarization.
    * [ ] Start the report with metrics, evaluation results, and create a diagram to visualize the evaluation process and proposed application.

